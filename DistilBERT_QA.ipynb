{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DistilBERT_QA",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNDvsXlDqHHrE20q7S1qDDS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sudarshansivakumar/SQuAD_QuestionAnswering/blob/main/DistilBERT_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85Z0oMxMnMVB"
      },
      "source": [
        "### **Installing and importing libraries** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8elPbtwrmAvf"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6zS3fl8nWra"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "from transformers import DistilBertTokenizer, DistilBertModel,PreTrainedTokenizer,PreTrainedTokenizerFast,BertModel,AutoTokenizer,DistilBertForQuestionAnswering\n",
        "import torch\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from tqdm.notebook import tqdm, trange\n",
        "import os\n",
        "from datasets import load_dataset\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKa8LAqcncdz"
      },
      "source": [
        "### **Loading dataset and dataloaders**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_lL2Ilp5Ruf"
      },
      "source": [
        "dataset = load_dataset(\n",
        "   'squad')\n",
        "train_ds = dataset['train']\n",
        "valid_ds = dataset['validation']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OWlVMRu5e0S"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCEieRZ75j7h"
      },
      "source": [
        "class BERTSQuADDataset(Dataset) :\n",
        "  def __init__(self,dset,tokenizer,max_length = 512,doc_stride = 0) :\n",
        "    self.dset = dset\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "    self.doc_stride = doc_stride\n",
        "\n",
        "  def __getitem__(self,idx) :\n",
        "    # Defining question and context strings\n",
        "    self.question = self.dset[idx]['question']\n",
        "    self.context = self.dset[idx]['context']\n",
        "    self.answer = self.dset[idx][\"answers\"][\"text\"][a0]\n",
        "    # Tokenizing question and context \n",
        "    tokenized_example = tokenizer(\n",
        "      self.question,\n",
        "      self.context,\n",
        "      max_length=self.max_length,\n",
        "      truncation=\"only_second\",\n",
        "      return_offsets_mapping=True,      \n",
        "      padding=\"max_length\"\n",
        "    )\n",
        "    \n",
        "    #tokenized_example[\"start_token\"] = []\n",
        "    #tokenized_example[\"end_token\"] = []\n",
        "    \n",
        "    offset_mapping = tokenized_example[\"offset_mapping\"]\n",
        "    input_ids = tokenized_example[\"input_ids\"]\n",
        "    attention_mask = tokenized_example[\"attention_mask\"]\n",
        "    answer_start_char = self.dset[idx][\"answers\"][\"answer_start\"][0]\n",
        "    answer_end_char = answer_start_char + len(self.answer)\n",
        "\n",
        "\n",
        "    \n",
        "    context_start_idx = 0\n",
        "    context_end_idx = len(offset_mapping) - 1\n",
        "    sequence_ids = tokenized_example.sequence_ids()\n",
        "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "    \n",
        "    while(sequence_ids[context_start_idx] != 1) :\n",
        "      context_start_idx += 1\n",
        "    while(sequence_ids[context_end_idx] != 1) :\n",
        "      context_end_idx -= 1\n",
        "    \n",
        "    if not (offset_mapping[context_start_idx][0] <= answer_start_char and offset_mapping[context_end_idx][1] >= answer_end_char) :\n",
        "      tokenized_example[\"start_token\"] = (cls_index)\n",
        "      tokenized_example[\"end_token\"] = (cls_index)\n",
        "    \n",
        "    else :\n",
        "      current_token = context_start_idx\n",
        "      gotStart,gotEnd = False,False\n",
        "\n",
        "      for start_char,end_char in (offset_mapping[context_start_idx : context_end_idx  + 1]) :  \n",
        "        if (start_char == answer_start_char) :\n",
        "          tokenized_example[\"start_token\"] = current_token\n",
        "          gotStart = True\n",
        "        if (end_char == answer_end_char) : \n",
        "          tokenized_example[\"end_token\"] = current_token\n",
        "          gotEnd = True\n",
        "        current_token += 1\n",
        "\n",
        "      if(gotStart == False) :\n",
        "        tokenized_example[\"start_token\"] = (cls_index)\n",
        "      if(gotEnd == False) :\n",
        "        tokenized_example[\"end_token\"] = (cls_index)\n",
        "       \n",
        "\n",
        "    return {\"Question\" : self.question, \n",
        "            \"Context\" : self.context, \n",
        "            \"Answer\" : self.answer,\n",
        "            \"Input_IDs\" : torch.tensor(tokenized_example[\"input_ids\"]),\n",
        "            \"Context_start_index\" : (context_start_idx),\n",
        "            \"Context_end_index\" : (context_end_idx),\n",
        "            \"Start_token\" : (tokenized_example[\"start_token\"]),\n",
        "            \"End_token\" : (tokenized_example[\"end_token\"]),\n",
        "            \"Offset_mapping\" : torch.tensor(tokenized_example[\"offset_mapping\"]),\n",
        "            \"Attention_mask\" : torch.tensor(tokenized_example[\"attention_mask\"])\n",
        "            }\n",
        "\n",
        "  def __len__(self) :\n",
        "    return len(self.dset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Fp1iMLy5pQN"
      },
      "source": [
        "BERTSQuAD_train = BERTSQuADDataset(train_ds,tokenizer,max_length = 512,doc_stride = 128)\n",
        "BERTSQuAD_valid = BERTSQuADDataset(valid_ds,tokenizer,max_length = 512,doc_stride = 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LS0jVpLy5sfM"
      },
      "source": [
        "BERTSQuAD_train_loader = DataLoader(BERTSQuAD_train,batch_size = 8,shuffle = True)\n",
        "BERTSQuAD_valid_loader = DataLoader(BERTSQuAD_valid,batch_size = 8,shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjShaAJq7_Rt"
      },
      "source": [
        "### **Model Training** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Y9yvXC8A1x"
      },
      "source": [
        "myPreTrainedQAModel = DistilBertForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "myPreTrainedOptimizer = torch.optim.AdamW(myPreTrainedQAModel.parameters(), lr = 5e-5)\n",
        "myPreTrainedQAModel = myPreTrainedQAModel.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjjyqUks8GuH"
      },
      "source": [
        "os.mkdir(\"PreTrained_QA_Model\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsjHknAG8PMS"
      },
      "source": [
        "def DistilPreTrainFT(model,train_loader,valid_loader,optimizer,num_epochs = 3,save_freq = 1,model_name = \"DistilBERT_SQuAD\",epoch_offset = 0,device = device,save_location = \"PreTrained_QA_Model\"):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss, running_f1,running_em = 0.0, 0.0,0.0\n",
        "        #current_loss,current_f1,current_em = 0.0,0.0,0.0\n",
        "        bar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "\n",
        "        for batch_idx,data in bar:\n",
        "          #data.to(device)\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Calculating model outputs\n",
        "          input_ids = data[\"Input_IDs\"].to(device)\n",
        "          attention_mask = data[\"Attention_mask\"].to(device)\n",
        "          start_positions = data[\"Start_token\"].to(device)\n",
        "          end_positions = data[\"End_token\"].to(device)\n",
        "          model_outputs = model(input_ids = input_ids,attention_mask = attention_mask,start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "          \n",
        "          # Storing model outputs, start_logits and end_logists have shape [batch_size,sequence_length]\n",
        "          batch_loss = model_outputs[0]\n",
        "          start_logits = model_outputs[1]\n",
        "          end_logits = model_outputs[2]\n",
        "            \n",
        "          # Getting the start and end labels and loading them on CUDA \n",
        "          start_labels = torch.unsqueeze(data['Start_token'],1)\n",
        "          end_labels = torch.unsqueeze(data['End_token'],1)\n",
        "          start_labels = start_labels.to(device)\n",
        "          end_labels = end_labels.to(device)  \n",
        "    \n",
        "          # Getting answers, context start, context end, adding them to CUDA \n",
        "          answers = data[\"Answer\"]\n",
        "          context_start_indices = data[\"Context_start_index\"]\n",
        "          context_end_indices = data[\"Context_end_index\"]\n",
        "            \n",
        "            \n",
        "          # Backpropagation through the loss   \n",
        "          batch_loss.backward()\n",
        "          # Updating the gradients \n",
        "          optimizer.step()\n",
        "        \n",
        "          #batch_size = start_logits.shape[0]  \n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "          # Getting start and end predictions for each instance in the batch \n",
        "          #batch_start_probs = torch.nn.Softmax(dim = 1)(start_logits)\n",
        "          #batch_end_probs = torch.nn.Softmax(dim = 1)(end_logits)\n",
        "          \n",
        "          #answer_predictions = []\n",
        "          #for i in range(batch_size) : \n",
        "            #instance_preds = model_outputs[i]\n",
        "            \n",
        "            #calculate answer_start_pred and answer_end_pred\n",
        "            #context_start = data[\"Context_start_index\"][i].item()\n",
        "            #context_end = data[\"Context_end_index\"][i].item()\n",
        "            #instance_start_probs = batch_start_probs[i][context_start:context_end + 1]\n",
        "            #instance_end_probs = batch_end_probs[i][context_start : context_end + 1]\n",
        "            #ans_start_token,ans_end_token,buffer = getAnsPred(instance_start_probs,instance_end_probs)\n",
        "            #ans_start_char = data[\"Offset_mapping\"][i][ans_start_token : ans_end_token + 1][0][0].item()\n",
        "            #ans_end_char = data[\"Offset_mapping\"][i][ans_start_token : ans_end_token + 1][-1][1].item()\n",
        "            #pred_answer = data[\"Context\"][i][ans_start_char : ans_end_char] \n",
        "            #answer_predictions.append(pred_answer)\n",
        "        \n",
        "\n",
        "          # So currently we have the actual answers looking like [\"mathematics and statistics\",\"Rafael Nadal\",....] and the predictions like [\"mathematics and statistics\",\"Nadal\",....]\n",
        "          #exact_matches = exact_match(answer_predictions,answers)\n",
        "          #f1_scores = f1_score(answer_predictions,answers)\n",
        "          \n",
        "          #batch_em = np.mean(exact_matches)\n",
        "          #batch_f1 = np.mean(f1_scores)\n",
        "          \n",
        "          running_loss += batch_loss.item()\n",
        "          #running_f1 += batch_f1\n",
        "          #running_em += batch_em\n",
        "        \n",
        "          bar.set_description(str({'epoch':epoch+1, \n",
        "                                   'Running loss': round((running_loss)/(batch_idx + 1),4), \n",
        "                                   }))\n",
        "\n",
        "          \"\"\"\n",
        "          running_loss += loss.item()\n",
        "          \n",
        "          #print(\"-\", end = \" \")\n",
        "          \"\"\"\n",
        "        epoch_training_loss = (running_loss)/len(train_loader)\n",
        "        #epoch_em = (running_em)/len(train_loader)\n",
        "        #epoch_f1 = (running_f1)/len(train_loader)\n",
        "        running_valid_loss = 0\n",
        "        try : \n",
        "          for data in valid_loader : \n",
        "            model.eval() \n",
        "            with torch.no_grad() : \n",
        "              input_ids = data[\"Input_IDs\"].to(device)\n",
        "              attention_mask = data[\"Attention_mask\"].to(device)\n",
        "              start_positions = data[\"Start_token\"].to(device)\n",
        "              end_positions = data[\"End_token\"].to(device)\n",
        "              model_outputs = model(input_ids = input_ids,attention_mask = attention_mask,start_positions=start_positions,\n",
        "                        end_positions=end_positions)\n",
        "              batch_loss = model_outputs[0]\n",
        "              running_valid_loss += batch_loss.item()\n",
        "          epoch_valid_loss = running_valid_loss/len(valid_loader)\n",
        "          print(f\"Epoch {epoch + epoch_offset + 1}, Epoch_training_loss : {epoch_training_loss}, Epoch_valid_loss : {epoch_valid_loss}\")\n",
        "        except : \n",
        "          print(f\"Epoch {epoch + epoch_offset + 1}, Epoch_training_loss : {epoch_training_loss} \")\n",
        "        if(epoch%save_freq == 0) : \n",
        "          try : \n",
        "            model.save_pretrained(save_location + \"/PreTrained_Model\" + str(epoch + 1))\n",
        "          except : \n",
        "            torch.save({\"params\": model.state_dict(),\"Epoch_loss\" : epoch_loss,\"Epoch_em\" : epoch_em,\"Epoch_F1\" : epoch_f1}, model_name + 'epoch'+str(epoch + epoch_offset)+'.pt')\n",
        "          \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5PSDOlM8SMQ"
      },
      "source": [
        "DistilPreTrainFT(model = myPreTrainedQAModel,train_loader = BERTSQuAD_train_loader,valid_loader =BERTSQuAD_valid_loader, optimizer = myPreTrainedOptimizer,num_epochs = 3,save_freq = 1,model_name = \"DistilBERT_SQuAD\",epoch_offset = 0,device = device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnwhOt63AfeG"
      },
      "source": [
        "### **Evaluating Model performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1XPD8__Ad-Y"
      },
      "source": [
        "def normalize_text(s):\n",
        "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
        "    import string, re\n",
        "\n",
        "    def remove_articles(text):\n",
        "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
        "        return re.sub(regex, \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "# Takes 2 lists of strings and compares corresponding elements to check if they are exact matches. \n",
        "def exact_match(preds,answer) : \n",
        "  exact_matches = []\n",
        "  for i in range(len(preds)) : \n",
        "    exact_matches.append(normalize_text(preds[i]) == normalize_text(answer[i]))\n",
        "  return exact_matches\n",
        "\n",
        "# Takes 2 lists of strings and calculates the F1 scores between corresponding elements in both strings\n",
        "def f1_score(preds,answer) : \n",
        "  f1_scores = []\n",
        "  for i in range(len(preds)) : \n",
        "    shared_words = 0\n",
        "    pred_words = normalize_text(preds[i]).split()\n",
        "    answer_words = normalize_text(answer[i]).split()\n",
        "    shared_words = set(pred_words) & set(answer_words)\n",
        "    try : \n",
        "      precision = (len(shared_words)/len(pred_words))\n",
        "    except : \n",
        "      precision = 0\n",
        "    try : \n",
        "      recall = (len(shared_words)/len(answer_words))\n",
        "    except :\n",
        "      recall = 0\n",
        "    \n",
        "    if(precision == 0 or recall == 0) : \n",
        "      f1_scores.append(0)\n",
        "    else : \n",
        "      f1_scores.append(2 * (precision * recall)/ (precision + recall))\n",
        "    #print(pred_words,answer_words,shared_words)\n",
        "  return f1_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZCWH9aYAlE5"
      },
      "source": [
        "# Getting the predicted answers for a given batch\n",
        "def getBatchPreds(example_batch,batch_start_probs,batch_end_probs,batch_size = 8) :\n",
        "  pred_answers = []\n",
        "  context_start_indices = example_batch[\"Context_start_index\"]\n",
        "  context_end_indices = example_batch[\"Context_end_index\"]\n",
        "  for i in range(batch_size) :\n",
        "    instance_start_probs,instance_end_probs = batch_start_probs[i],batch_end_probs[i]\n",
        "    context_start,context_end = context_start_indices[i],context_end_indices[i]\n",
        "    offset_maps = example_batch[\"Offset_mapping\"][i]\n",
        "    context = example_batch[\"Context\"][i]\n",
        "    best_start,best_end,best_prob = context_start,context_start,instance_start_probs[context_start] * instance_end_probs[context_start]\n",
        "    for j in range(context_start,context_end + 1) : \n",
        "      for k in range(j,context_end + 1) : \n",
        "        current_prob = instance_start_probs[j] * instance_end_probs[k]\n",
        "        if(current_prob > best_prob) : \n",
        "          best_start,best_end,best_prob = j,k,current_prob\n",
        "    start_char = offset_maps[best_start][0]\n",
        "    end_char = offset_maps[best_end][1]\n",
        "    ans = context[start_char:end_char]\n",
        "    pred_answers.append(ans)\n",
        "\n",
        "  return pred_answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEEHZIZBBI8q"
      },
      "source": [
        "# Calculating overall exact match and F1 score of the entire model on the given dataloader\n",
        "def getAccuracies(loader,model) :\n",
        "  avg_em, avg_f1 = 0,0\n",
        "  bar = tqdm(enumerate(loader), total=len(loader)) \n",
        "  for batch_idx,batch in bar : \n",
        "    batch_start_probs,batch_end_probs = getOutputs(batch,model)\n",
        "    pred_answers = getBatchPreds(batch,batch_start_probs,batch_end_probs)\n",
        "    actual_answers = batch[\"Answer\"]\n",
        "    em = np.mean(exact_match(pred_answers,actual_answers))\n",
        "    f1 = np.mean(f1_score(pred_answers,actual_answers))\n",
        "    avg_em += em\n",
        "    avg_f1 += f1\n",
        "    bar.set_description(str({'running_em' : (avg_em/(batch_idx + 1)),\n",
        "                             'running_f1' : (avg_f1/(batch_idx + 1))}))\n",
        "  avg_em = (avg_em)/(len(loader))\n",
        "  avg_f1 = (avg_f1)/(len(loader))\n",
        "  return avg_em,avg_f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "covrw5B7BWSN"
      },
      "source": [
        "train_em,train_f1 = getAccuracies(BERTSQuAD_train_loader,myPreTrainedQAModel)\n",
        "valid_em,valid_f1 = getAccuracies(BERTSQuAD_valid_loader,myPreTrainedQAModel)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}